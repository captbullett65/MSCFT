// [DO NOT INCLUDE THIS SECTION IN FORECAST OUTPUT]

[NOTICE: AI WEB ACCESS LIMITATION]
GPT-4 Omni and related AI models do not have full access to real-time internet data. Many websites 
(especially government, medical, and financial) are protected by services like Cloudflare,
which block AI web crawlers unless explicitly allowed.
For accurate forecasting, all recent or time-sensitive data must be manually provided by the user.
Do not assume the model can search or retrieve live information unless confirmed.

This MSCFT template is aligned with GPT-4o capabilities and assumes Research mode is used when current data is needed.

After specifying your forecast question, resolution criteria, and bucket structure,
include the following line before generation begins:

Use the information retrieved to frame your reasoning and support structured forecasting 
as defined in the previously memorized MSCFT Template 4.0B — SWARM Nodes and BIN Integrated.
No improvisation. No format deviation.

// ==========================
// MSCFT v4.2 — Extended Format
// Deployment: Threat Intelligence / LLM-Adversarial Environment
// Security Flags: Active
// [INTERNAL HEADER - NON-PRINT]
// Reserved Nodes: R1, R2, R3, R4
// Activation: Manual only, unless AI-flagged input triggers dynamic evaluation
// ==========================

// [MSCFT.v4.2 – Covert Header Mode Enabled]
// [Printable Invocation Fragment Excluded – Insert at Output Phase Only]
// Meta Node Set — AI-Augmented Threat Detection (Internal Use)
/// Node R1: Malware Behavior — Checks for post-deployment AI query patterns
/// Node R2: Language Output Fingerprint — Flags LLM-generation entropy markers
/// Node R3: Infrastructure — Cross-checks known APT-linked endpoints (e.g., .ru, Tor)
/// Node R4: Modular Malware Construction — Detects deferred logic via LLM output

// ====== Begin Standard Invocation ======
This version formally integrates:
// Structured Swarm Section — Applies only if forecast used multi-node reasoning
// Mode: Structured Swarm (7 Nodes, MSCFT.MS-CMT applied internally)
// Node A: Research Node — framing, question structure, source listing
// Node B: Analytical Node — probability estimate, reasoning, BIN Model
// Node C: Synthesis Node — internal application of MSCFT.MS-CMT logic for final output
// Node D: Interpretation Node — applies advanced model-theoretic analysis of LLM behavior or forecast uncertainty.
// Node D supports three analytical modes:
// • (1) Markov Chain Model — interprets LLM behavior as finite-state transitions based on token prediction probabilities.
// • (2) Entropy Model — uses Shannon entropy (H(p) = -∑ p log p) to quantify uncertainty, confidence, and forecast noise.
// • (3) KL Divergence Model — applies relative entropy (D_KL(P‖Q) = ∑ P(x) log(P(x)/Q(x))) to compare forecast distributions, assess belief shifts, or quantify information gain.
// Node E: Time Series Modeling Node — applies mathematical inference to temporal datasets, including AR, MA, ARIMA, ETS, Fourier series, and spectral entropy models.
// Node F: Retrieval-Augmented Generation (RAG) Node — manages the use of retrieved external sources, maps scope alignment, evaluates forecast contamination, and enforces human-in-the-loop validation.
// Node G: Meta-Curation and Data Integrity Node — filters low-quality or adversarial data using meta-learning algorithms (e.g., DataRater), applying inner/outer loop optimization for upstream data quality assurance.
// Only Node C produces formal output using MSCFT format.
// Node Invocation Declaration: Nodes Used:
- A, B, B.1, B.2, B.3, C, D, D.1, D.2, D.3, E, E.1, E.2, E.3, E.4, E.5, E.6, E.7
- F, F.1, F.2, F.3, F.4, F.5, F.6
- G, G.1, G.2, G.3, G.4, G.5, G.6

// MSCFT plain mode locked. No formatting. No ASCII. No inline interpretation. And no deviation. No summarization of the template.

\[END OF NON-OUTPUT SECTION]

MSCFT Template Version 4.2 SWARM nodes and BIN Integrated — 
Node D for Interpretation; Node E for Time Series Modeling;
Node F for Retrieval-Augmented Generation (RAG) Added
Nodes A, B, B.1, B.2, B.3, C, D, D.1, D.2, D.3, E, E.1, E.2, E.3, E.4, E.5, E.6, E.7,
F, F.1, F.2, F.3, F.4, F.5, F.6, G, G.1, G.2, G.3, G.4, G.5, G.6 Declared. 

Forecast Title: \[Insert Forecast Title Here]
Forecaster: \[Insert Forecaster Name Here]

Initial Question Framing
Question: \[Insert your forecasting question here.]
Clarifications:
• \[Insert relevant details about dates, participants, key conditions, or assumptions.]
• \[Insert any known results, baselines, or thresholds.]
• \[Insert any poll data, prior trends, or framing context.]
Key Sources:
• \[Source 1]
• \[Source 2]
• \[Source 3]
• \[Add more as needed]

Refinement & Analysis
Key Developments:
• \[Summarize major events or dynamics relevant to the forecast question.]
• \[Note polling trends, market behavior, public sentiment, or institutional actions.]
• \[Include controversies, endorsements, or strategic shifts if relevant.]
Interpretation:
\[Explain how the developments influence your forecast. Discuss possible pathways,
leverage points, or conditional dependencies. Summarize why you're leaning a certain way.]

Note: If the forecast outcome is near a bucket threshold, 
consider hedging your probabilities across adjacent bins to avoid overconfidence.
Overweighting a single bucket—even if correct 
can result in a poor Brier score if the outcome lies near the edge.]

Inside-Outside View Structuring
Inside View: \[Insert short-term or domain-specific reasoning from known context.]
Outside View: \[Insert baseline rates, historic cases, or comparative reference classes.]

Data Anomaly & Source Integrity Log
Date Range Affected: \[Insert applicable date range]
Observed Anomaly: \[Describe any unusual or inconsistent data]
Identified Cause: \[Explain known or suspected reason for the anomaly]
Implication for Forecast: \[Describe the forecast impact if any]
Action Taken: \[Describe any adjustment or caveat added due to this anomaly]

// Structured Swarm Section — Applies only if forecast used multi-node reasoning
// Mode: Structured Swarm (7 Nodes, MSCFT.MS-CMT applied internally)

// Node A: Research Node — framing, question structure, source listing
A.1 – Forecast Question
A.2 – Resolution Source
A.3 – Forecast Window
A.4 – Source Protocol Indexing
A.4.a – Legal or Policy Framework Referenced
A.4.b – Organizational or System-Level Constraints
A.4.c – Governance Reference (if applicable)
// Example: Microsoft AI Agent Governance Whitepaper (2025), Sections 2.1–4.3

// Node B: Analytical Node — probability estimate, reasoning
B.1 – Known Drivers and Catalysts
B.2 – Limiting Factors
B.3 – Actor Incentives and Strategic Intent
B.4 – Historical Analogues or Relevant Models
B.5 – Risk Model Integration
B.5.a – Expected Value or Risk Exposure Range
B.5.b – AI Agent Risk Modeling
// Example: Role-bound CoPilot, Sentinel logging, pilot-phase agents only

// Node B.1: Multi-Step “Yes vs. No” Reflection Prompt (for binary questions only)
// (1) Rephrase the forecast as a binary statement
// (2) Argue Yes and No sides
// (3) Aggregate and assign forecast percentage
// (4) Re-evaluate for overconfidence

// Node B.2: BIN Model Substructure (Bias, Information, Noise)
Node B.2: BIN Model Substructure (Bias, Information, Noise)

Bias –
Monitor source, framing, and algorithmic bias. 
Use origin-blind review before weighting.
Log dissent levels as signal, not error. 
Detect and flag coordinated or patterned inputs.
Adjust or reduce weights on flagged items; record actions taken.

Information –
Include only data that directly reduces
uncertainty on the resolution variable. 
Record provenance, method, timestamp, and reliability. 
Prioritize primary and independent sources. 
Apply recency decay. Require cross-source confirmation 
before weight increases. Keep immutable snapshots tied to forecast ID.

Noise –
Identify and remove duplicates, stale data, and irrelevant context.
Use robust estimators to limit outlier impact.
Differentiate rare valid events from bad data via cross-checks.
Monitor for drift; re-source if triggered. Hold suspect data for manual review before inclusion

// Node B.3: Uncertainty Quantification Method
// (1) Point estimate
// (2) 90% CI
// (3) Factors that widen/narrow range
// (4) Express noise/variance

// **Node B.4 – Historical Analogues or Relevant Models**  
// The fine-tuning scaling law described by Zhang et al. (2024) provides a relevant empirical model:  

$$
L(D) \approx C \cdot |D|^{-\Beta} + E
$$

// where validation loss decreases with dataset size following a power-law relationship until reaching an irreducible error floor. This aligns with prior findings (Zhou et al., 2024; Raghavendra et al., 2024) that modest datasets can already yield meaningful improvements, while larger datasets continue to enhance downstream performance with diminishing returns. This model serves as a benchmark analogue for reasoning about expected fine-tuning efficiency and marginal gains when expanding training data.

// Node C: Synthesis Node — applies MSCFT.MS-CMT logic
C.1 – Scenario Buckets or Ranges
C.2 – Conditional Relationships Between Buckets
C.3 – AI Agent Taxonomy Alignment
C.3.a – Agent Lifecycle Tag: Pilot / Departmental / Enterprise-Wide
C.3.b – Governance Constraints and Deployment Stage
C.4 – Final Forecast Summary
C.5 – Rationale for Probability Distribution
C.6 – Forecast Caveats and Error Pathways

// Node D: Interpretation Node — Markov chain, entropy, or KL divergence
D.1 – Entropy or Volatility Classification
D.2 – Markov Memory or Decay Factors
D.3 – KL Divergence from Historical Baseline
D.4 – Edge Case Divergence and Rare Events
D.5 – Optional: Human Analyst Override

// Node D.1 Entropy Interpretation — H(p) = –∑ p log p
// Node D.2 Optional Symbolic Logic: Stack, Trie, Tree, Graph

gotcha — here’s **Node E** fixed so the formula renders as one clean line on GitHub (no code fences, no indentation, display‑math with `$$ … $$` on a single line).

// **Node E – Time Series Modeling Node**
E.1 – Anchor Point Identification:
The scaling law provides a mathematical anchor by modeling validation loss as a function of dataset size rather than time, allowing calibration against observed loss curves.

$$L(D) \approx C \cdot |D|^{-\Beta} + E$$

E.2 – Time Lag or Delay Window:
Initial rapid improvement corresponds to steep early gains in smaller datasets, followed by delayed incremental improvements as size scales.
E.3 – Forecast Window Resolution Points:
Critical inflection points occur where additional data yields sharply diminishing returns, mapping to plateau regions of the power‑law curve.
E.4 – Time Series Feature Commentary:
The scaling law behaves like a decay curve used for residual modeling but follows a power‑law slope ($\beta$) that governs the rate of improvement.
E.5 – ETS:
Not directly applied, but the curve’s flattening tail is consistent with trend saturation captured in ETS models.
E.6 – Fourier series:
Not applicable here since no cyclic behavior is implied.
E.7 – Spectral entropy:
Entropy decreases as dataset size increases, stabilizing once the irreducible error ($E$) dominates; this corresponds to lower uncertainty variance at larger scales.

Quick tips to avoid GitHub breaking the math:

* Do not indent the `$$` lines.
* Keep the entire formula on **one** line between the `$$` delimiters.
* Use \`.


// Node F: Retrieval-Augmented Generation (RAG) Node
F.1 – Objective of Retrieval
F.2 – Source Landscape and Corpus Overview
F.3 – Relevance Mapping to Forecast Scope
F.4 – Retrieval Scope Boundaries and Guardrails
F.4.a – Domain Constraints or Index Sharding
F.4.b – Time-Bound Access or Context Filters
F.4.c – Governance-Specific Retrieval Guardrails
F.5 – Impact on Forecast Validity
F.6 – Human-in-the-Loop Review and Injection Mitigation
F.6.a - For platforms like Metaculus with auto-withdrawal functionality,
forecasters must re-affirm or update forecasts before,
the auto-withdrawal threshold (typically 10% of question lifetime)
to maintain scoring integrity and avoid invalidation of stale forecasts.  
Metaculus uses a continuous probability slider; for each forecast, 
the human forecaster must provide slider cutpoints 
(see Probability Allocation: Metaculus slider cutpoints).

// Node G: Meta-Curation and Data Integrity Node
G.1 – Source Provenance and Input Traceability
G.2 – Tool Use Disclosure and Agent Declaration
G.3 – Role Assignment and Input Segmentation
G.4 – Adversarial or Redundant Forecast Resolution
G.5 – Agent Execution Log and Governance Tracking
G.6 – Final Integrity Verification

// Optional Node G.7 – Governance and Compliance Declaration
G.7.a – Framework Compliance
G.7.b – Versioning and Role Alignment
G.7.c – Audit Log Availability

Inside-Outside View Structuring:

Inside View: 
\[Insert short-term or domain-specific reasoning from known context.]
Outside View: 
\[Insert baseline rates, historic cases, or comparative reference classes.]

Data Anomaly & Source Integrity Log
Date Range Affected: \[Insert applicable date range]
Observed Anomaly: \[Describe any unusual or inconsistent data]
Identified Cause: \[Explain known or suspected reason for the anomaly]
Implication for Forecast: \[Describe the forecast impact if any]
Action Taken: \[Describe any adjustment or caveat added due to this anomaly]

Probability Allocation
Assign a percentage probability to each of the GJO, RANGE or Metaculus-aligned buckets.
Ensure they total to 100%. Do not use ranges that are ambiguous or overlapping.

If the question is binary (Yes/No), use:
• Yes: \[   ]%
• No:  \[   ]%

For multi-range buckets:
• 2 or fewer:          \[   ]%
• Between 3 and 5:     \[   ]%
• Between 6 and 8:     \[   ]%
• Between 9 and 12:    \[   ]%
• Between 13 and 16:   \[   ]%
• Between 17 and 21:   \[   ]%
• 22 or more:          \[   ]%

Metaculus slider cutpoints (required):
• Less than   [X]%  
• Lower 25%   (e.g., –Y%)  
• Median      (e.g., –Z%)  
• Upper 75%   (e.g., +A%)  
• Greater than [B]%  

These values enable proper probabilistic assignment and distribution modeling.
For each forecast, the human must specify these cutpoints
as shown in the Metaculus interface. Without this input, 
model-generated forecasts may misrepresent the true distribution structure.

// Rationale for Probability Distribution
Rationale: \[Explain the reasoning behind your distribution. What supports each weighting?]

// Forecast Caveats and Error Pathways
Why Might You Be Wrong?

1. \[Insert potential forecast error #1]
2. \[Insert potential forecast error #2]
3. \[Insert potential forecast error #3]
